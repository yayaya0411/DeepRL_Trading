{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import logging\n",
    "from utils import *\n",
    "\n",
    "from importlib import reload\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import *\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hold(actions):\n",
    "    # encourage selling for profit and liquidity\n",
    "    next_probable_action = np.argsort(actions)[1]\n",
    "    if next_probable_action == 2 and len(agent.inventory) > 0:\n",
    "        max_profit = stock_prices[t] - min(agent.inventory)\n",
    "        if max_profit > 0:\n",
    "            sell(t)\n",
    "            actions[next_probable_action] = 1 # reset this action's value to the highest\n",
    "            return 'Hold', actions\n",
    "\n",
    "def buy(t):\n",
    "    if agent.balance > stock_prices[t]:\n",
    "        agent.balance -= stock_prices[t]\n",
    "        agent.inventory.append(stock_prices[t])\n",
    "        return 'Buy: ${:.2f}'.format(stock_prices[t])\n",
    "\n",
    "def sell(t):\n",
    "    if len(agent.inventory) > 0:\n",
    "        agent.balance += stock_prices[t]\n",
    "        bought_price = agent.inventory.pop(0)\n",
    "        profit = stock_prices[t] - bought_price\n",
    "        global reward\n",
    "        reward = profit\n",
    "        return 'Sell: ${:.2f} | Profit: ${:.2f}'.format(stock_prices[t], profit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved_models\\DQN_16_dim.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'DQN'\n",
    "stock_name = '0050_2008_2018'\n",
    "window_size = 10\n",
    "num_episode = 5\n",
    "initial_balance = 50000\n",
    "\n",
    "stock_prices = stock_close_prices(stock_name)\n",
    "stock_margin = stock_margins(stock_name)\n",
    "trading_period = len(stock_prices) - 1  # 訓練期間，input stock data的總日期\n",
    "returns_across_episodes = []\n",
    "num_experience_replay = 0\n",
    "action_dict = {0: 'Hold', 1: 'Buy', 2: 'Sell'}\n",
    "\n",
    "logging.basicConfig(filename=f'logs/{model_name}_training_{stock_name}.log', filemode='w',\n",
    "                    format='[%(asctime)s.%(msecs)03d %(filename)s:%(lineno)3s] %(message)s', \n",
    "                    datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO)\n",
    "\n",
    "logging.info(f'Trading Object:           {stock_name}')\n",
    "logging.info(f'Trading Period:           {trading_period} days')\n",
    "logging.info(f'Window Size:              {window_size} days')\n",
    "logging.info(f'Training Episode:         {num_episode}')\n",
    "logging.info(f'Model Name:               {model_name}')\n",
    "logging.info('Initial Portfolio Value: ${:,}'.format(initial_balance))\n",
    "\n",
    "# select learning model\n",
    "model = importlib.import_module(f'agents.{model_name}')\n",
    "agent = model.Agent(state_dim=13 + 3, balance=initial_balance,model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "import utils\n",
    "reload(utils)\n",
    "from utils import *\n",
    "\n",
    "for e in tqdm.tqdm(range(1, num_episode + 1)):\n",
    "    logging.info(f'\\nEpisode: {e}/{num_episode}')\n",
    "\n",
    "    agent.reset() # reset to initial balance and hyperparameters\n",
    "    state = generate_combined_state(0, window_size, stock_prices, stock_margin, agent.balance, len(agent.inventory)) \n",
    "    # 將prince_state與portfolio_state 橫向串連起來橫向串連，作為input state\n",
    "\n",
    "    for t in range(1, trading_period + 1):\n",
    "        if t % 1000 == 0:\n",
    "            logging.info(f'\\n-------------------Period: {t}/{trading_period}-------------------')\n",
    "\n",
    "        reward = 0\n",
    "        next_state = generate_combined_state(t, window_size, stock_prices, stock_margin, agent.balance, len(agent.inventory))\n",
    "        # display(pd.DataFrame(next_state))\n",
    "        previous_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n",
    "        # print(t,'\\ninventory',agent.inventory,'\\ninventory len',len(agent.inventory),'\\nstock prices',stock_prices[t], \\\n",
    "        #     '\\ninventory*stockprices + balance = ',len(agent.inventory)*stock_prices[t],'+', agent.balance,\\\n",
    "        #     '\\nprevious portfolio value',previous_portfolio_value,'\\n')\n",
    "        \n",
    "        if model_name == 'DDPG':\n",
    "            actions = agent.act(state, t)\n",
    "            action = np.argmax(actions)\n",
    "        else:\n",
    "            actions = agent.model.predict(state)[0]\n",
    "            action = agent.act(state)\n",
    "            \n",
    "        # execute position\n",
    "        logging.info('Step: {}\\tHold signal: {:.4} \\tBuy signal: {:.4} \\tSell signal: {:.4}'.format(t, actions[0], actions[1], actions[2]))\n",
    "        if action != np.argmax(actions): logging.info(f\"\\t\\t'{action_dict[action]}' is an exploration.\")\n",
    "        if action == 0: # hold\n",
    "            execution_result = hold(actions)\n",
    "        if action == 1: # buy\n",
    "            execution_result = buy(t)      \n",
    "        if action == 2: # sell\n",
    "            execution_result = sell(t)        \n",
    "        \n",
    "        # check execution result\n",
    "        if execution_result is None:\n",
    "            reward -= treasury_bond_daily_return_rate() * agent.balance  # missing opportunity\n",
    "        else:\n",
    "            if isinstance(execution_result, tuple): # if execution_result is 'Hold'\n",
    "                actions = execution_result[1]\n",
    "                execution_result = execution_result[0]\n",
    "            logging.info(execution_result)    \n",
    "                        \n",
    "        # calculate reward\n",
    "        current_portfolio_value = len(agent.inventory) * stock_prices[t] + agent.balance\n",
    "        unrealized_profit = current_portfolio_value - agent.initial_portfolio_value\n",
    "        reward += unrealized_profit\n",
    "\n",
    "        agent.portfolio_values.append(current_portfolio_value)\n",
    "        agent.return_rates.append((current_portfolio_value - previous_portfolio_value) / previous_portfolio_value)\n",
    "\n",
    "        done = True if t == trading_period else False\n",
    "        agent.remember(state, actions, reward, next_state, done)\n",
    "\n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "        # experience replay\n",
    "        if len(agent.memory) > agent.buffer_size:\n",
    "            num_experience_replay += 1\n",
    "            loss,mini_batch = agent.experience_replay()\n",
    "            logging.info('Episode: {}\\tLoss: {:.2f}\\tAction: {}\\tReward: {:.2f}\\tBalance: {:.2f}\\tNumber of Stocks: {}'.format(e, loss, action_dict[action], reward, agent.balance, len(agent.inventory)))\n",
    "            agent.tensorboard.on_batch_end(num_experience_replay, {'loss': loss, 'portfolio value': current_portfolio_value})\n",
    "        if done:\n",
    "            portfolio_return = evaluate_portfolio_performance(agent, logging)\n",
    "            returns_across_episodes.append(portfolio_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ef9229\\AppData\\Local\\Temp\\ipykernel_690960\\3510162282.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  np.array(sample).shape\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "sample_index = random.sample(range(29),10)\n",
    "sample = [mini_batch[i] for i in sample_index]\n",
    "np.array(sample).shape\n",
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if model_name == 'DQN':\n",
    "        agent.model.save(os.path.join(f'saved_models',f'{model_name}_{agent.state_dim}_dim.h5'))\n",
    "    elif model_name == 'DDQN':\n",
    "        agent.model.save('saved_models/DDQN_ep' + str(e) + '.h5')\n",
    "        agent.model_target.save('saved_models/DDQN_ep' + str(e) + '_target.h5')\n",
    "    elif model_name == 'DDPG':\n",
    "        agent.actor.model.save_weights('saved_models/DDPG_ep{}_actor.h5'.format(str(e)))\n",
    "        agent.critic.model.save_weights('saved_models/DDPG_ep{}_critic.h5'.format(str(e)))\n",
    "    logging.info('model saved')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9404e1fbfb1de12c98a183962fea4f9ddc98635384508580938d531b4dfff0d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('DeepRL_Trading-ppLIFqgY': pipenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
